{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c26839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from Models import MoELSTM\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "import random\n",
    "from Models import MoELSTM, LSTMModel, train_model\n",
    "from Preprocess import (\n",
    "    compute_metrics,\n",
    "    convert_timeseries_to_numpy,\n",
    "    create_dataloader,\n",
    "    load_building_series,\n",
    "    split_series_list,\n",
    ")\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from Models import model_fn\n",
    "from tqdm import tqdm\n",
    "from my_utils import train_model, load_energy_data_feather, get_weights, set_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d0695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AggregationStrategy import sync_aggregate,average_weights,sync_aggregate_norm,sync_aggregate_softmax, fedavgm_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf3f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(\"train_final.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a5334f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>primary_use</th>\n",
       "      <th>air_temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7593144</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-21 01:00:00</td>\n",
       "      <td>72.221012</td>\n",
       "      <td>Education</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7593145</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-21 01:00:00</td>\n",
       "      <td>39.611586</td>\n",
       "      <td>Education</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7593146</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-21 01:00:00</td>\n",
       "      <td>1.920567</td>\n",
       "      <td>Education</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7593147</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-21 01:00:00</td>\n",
       "      <td>111.532464</td>\n",
       "      <td>Education</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7593148</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-21 01:00:00</td>\n",
       "      <td>456.734799</td>\n",
       "      <td>Education</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         building_id  meter           timestamp  meter_reading primary_use  \\\n",
       "7593144            0      0 2016-05-21 01:00:00      72.221012   Education   \n",
       "7593145            1      0 2016-05-21 01:00:00      39.611586   Education   \n",
       "7593146            2      0 2016-05-21 01:00:00       1.920567   Education   \n",
       "7593147            3      0 2016-05-21 01:00:00     111.532464   Education   \n",
       "7593148            4      0 2016-05-21 01:00:00     456.734799   Education   \n",
       "\n",
       "         air_temperature  \n",
       "7593144             25.6  \n",
       "7593145             25.6  \n",
       "7593146             25.6  \n",
       "7593147             25.6  \n",
       "7593148             25.6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1317d9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11712248 entries, 7593144 to 20216099\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Dtype         \n",
      "---  ------           -----         \n",
      " 0   building_id      int64         \n",
      " 1   meter            int64         \n",
      " 2   timestamp        datetime64[ns]\n",
      " 3   meter_reading    float64       \n",
      " 4   primary_use      object        \n",
      " 5   air_temperature  float64       \n",
      "dtypes: datetime64[ns](1), float64(2), int64(2), object(1)\n",
      "memory usage: 625.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97aecb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Config\n",
    "# List of models to experiment with\n",
    "MODEL_NAMES = [\"transformer\"]\n",
    "\n",
    "# Config\n",
    "NUM_CLIENTS = 1410\n",
    "CLIENT_FRAC = 0.15\n",
    "NUM_ROUNDS = 50\n",
    "LOCAL_EPOCHS = 5\n",
    "LR = 0.001\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_FILE =\"train_final.feather\" # \"meter_0_data_cleaned.feather\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a022de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_energy_data_feather_transformer(cid, filepath=\"meter_0_data_cleaned.feather\"):\n",
    "    \"\"\"Load, preprocess, and return train/test dataloaders for a client.\"\"\"\n",
    "    df = pd.read_feather(filepath)\n",
    "    df = df[df['building_id'] == cid]\n",
    "    df['meter_reading'] = df['meter_reading'].fillna(0)\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No data found for building_id {cid}\")\n",
    "\n",
    "    try:\n",
    "        ts = TimeSeries.from_dataframe(\n",
    "            df,\n",
    "            time_col='timestamp',\n",
    "            value_cols='meter_reading',\n",
    "            fill_missing_dates=True,\n",
    "            freq='h'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to construct TimeSeries: {e}\")\n",
    "\n",
    "    train_series, test_series = ts.split_before(0.75)\n",
    "\n",
    "    if len(train_series) == 0 or len(test_series) == 0:\n",
    "        raise ValueError(f\"Empty time series for building_id {cid}. Train: {len(train_series)}, Test: {len(test_series)}\")\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0.1, 1))\n",
    "    transformer = Scaler(scaler)\n",
    "    transformed_train_series = transformer.fit_transform(train_series)\n",
    "    # transformed_test_series = transformer.transform(test_series)\n",
    "\n",
    "    X_train, y_train = convert_timeseries_to_numpy(transformed_train_series, input_len=168, output_len=24)\n",
    "    # X_test, y_test = convert_timeseries_to_numpy(transformed_test_series, input_len=168, output_len=24)\n",
    "\n",
    "    X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0)\n",
    "    # X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "    # y_test = np.nan_to_num(y_test, nan=0.0)\n",
    "\n",
    "\n",
    "    if len(X_train) == 0 :#or len(X_test) == 0:\n",
    "        raise ValueError(f\"Client {cid} has no data after preprocessing.\")\n",
    "\n",
    "    train_loader = create_dataloader(X_train, y_train, batch_size=256)\n",
    "    # test_loader = create_dataloader(X_test, y_test, batch_size=256)\n",
    "\n",
    "    return train_loader, 0 # test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "590bf743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding2(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # (d_model/2)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5569052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_length: int = 168,\n",
    "        forecast_horizon: int = 24,\n",
    "        d_model: int = 128,\n",
    "        n_heads: int = 8,\n",
    "        n_layers: int = 4,\n",
    "        d_ff: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        # 1. Input projection: from 1 → d_model\n",
    "        self.input_embedding = nn.Linear(1, d_model)\n",
    "\n",
    "        # 2. Forecast tokens (learnable embeddings)\n",
    "        self.forecast_tokens = nn.Parameter(torch.randn(1, forecast_horizon, d_model))\n",
    "\n",
    "        # 3. Positional encoding for (context + forecast) steps\n",
    "        self.pos_encoding = PositionalEncoding2(d_model, max_len=context_length + forecast_horizon)\n",
    "\n",
    "        # 4. Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Makes all inputs/outputs (batch, seq, dim)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # 5. Output projection: d_model → 1\n",
    "        self.output_projection = nn.Linear(d_model, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, context_length, 1)\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, forecast_horizon, 1)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # 1. Project input from (1) to (d_model)\n",
    "        x = self.input_embedding(x)  # (batch_size, context_length, d_model)\n",
    "\n",
    "        # 2. Expand forecast tokens\n",
    "        forecast_tokens = self.forecast_tokens.expand(batch_size, -1, -1)  # (batch_size, forecast_horizon, d_model)\n",
    "\n",
    "        # 3. Concatenate context + forecast tokens\n",
    "        full_input = torch.cat([x, forecast_tokens], dim=1)  # (batch_size, context + horizon, d_model)\n",
    "\n",
    "        # 4. Add positional encoding\n",
    "        full_input = self.pos_encoding(full_input)\n",
    "\n",
    "        # 5. Causal mask (prevents forecast tokens from attending to future)\n",
    "        seq_len = full_input.size(1)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
    "\n",
    "        # 6. Transformer encoder\n",
    "        transformer_output = self.encoder(full_input, mask=causal_mask)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # 7. Slice only forecast part\n",
    "        forecast_output = transformer_output[:, -self.forecast_horizon:, :]  # (batch_size, forecast_horizon, d_model)\n",
    "\n",
    "        # 8. Project back to 1 value per timestep\n",
    "        output = self.output_projection(self.dropout(forecast_output))  # (batch_size, forecast_horizon, 1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5188c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDifficultyWeight:\n",
    "    def __init__(self, num_clients, accumulate_iters=20):\n",
    "        self.num_clients = num_clients\n",
    "        self.last_loss = torch.ones(num_clients).float().to(DEVICE)\n",
    "        self.learn_score = torch.zeros(num_clients).float().to(DEVICE)\n",
    "        self.unlearn_score = torch.zeros(num_clients).float().to(DEVICE)\n",
    "        self.ema_difficulty = torch.ones(num_clients).float().to(DEVICE)\n",
    "        self.accumulate_iters = accumulate_iters\n",
    "\n",
    "    def update(self, cid: int, loss_history: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        Update difficulty based on loss trend for a client.\n",
    "        Expects a list of per-epoch losses.\n",
    "        \"\"\"\n",
    "        current_loss = torch.tensor(loss_history[-1], dtype=torch.float32).to(DEVICE)\n",
    "        previous_loss = self.last_loss[cid]\n",
    "        delta = current_loss - previous_loss\n",
    "        ratio = torch.log((current_loss + 1e-8) / (previous_loss + 1e-8))\n",
    "\n",
    "        learn = torch.where(delta < 0, -delta * ratio, torch.tensor(0.0, device=current_loss.device))\n",
    "        unlearn = torch.where(delta >= 0, delta * ratio, torch.tensor(0.0, device=current_loss.device))\n",
    "\n",
    "        # EMA update\n",
    "        momentum = (self.accumulate_iters - 1) / self.accumulate_iters\n",
    "        self.learn_score[cid] = momentum * self.learn_score[cid] + (1 - momentum) * learn\n",
    "        self.unlearn_score[cid] = momentum * self.unlearn_score[cid] + (1 - momentum) * unlearn\n",
    "\n",
    "        # Difficulty score\n",
    "        diff_ratio = (self.unlearn_score[cid] + 1e-8) / (self.learn_score[cid] + 1e-8)\n",
    "        difficulty = diff_ratio #torch.pow(diff_ratio, 1 / 5)\n",
    "\n",
    "        # Smooth difficulty over rounds\n",
    "        self.ema_difficulty[cid] = momentum * self.ema_difficulty[cid] + (1 - momentum) * difficulty\n",
    "\n",
    "        self.last_loss[cid] = current_loss\n",
    "        return self.ema_difficulty[cid].item()\n",
    "\n",
    "    def get_normalized_weights(self, client_ids: List[int]) -> List[float]:\n",
    "        weights = [self.ema_difficulty[cid].item() for cid in client_ids]\n",
    "        total = sum(weights)\n",
    "        if total == 0:\n",
    "            return [1.0 / len(client_ids)] * len(client_ids)\n",
    "        return [w / total for w in weights]\n",
    "    \n",
    "    def get_sampling_probabilities(self, min_prob=0.05):\n",
    "        difficulty = self.ema_difficulty\n",
    "        inv_difficulty = 1.0 / (difficulty + 1e-6)\n",
    "        inv_difficulty = inv_difficulty / inv_difficulty.sum()\n",
    "        probs = torch.clamp(inv_difficulty, min=min_prob)\n",
    "        return (probs / probs.sum()).cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "965c1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model_transformer(model, train_loader, device=None, learning_rate=0.001, loss_fn=None, optimizer_class=optim.Adam, epochs=50):\n",
    "    \"\"\"Train the model and return the average loss.\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    loss_fn = loss_fn or nn.MSELoss()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            # if y_batch.dim() == 3 and y_batch.shape[-1] == 1:\n",
    "            #     y_batch = y_batch.squeeze(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = loss_fn(output.squeeze(-1), y_batch)\n",
    "            # loss = loss_fn(output.squeeze(-1), y_batch)  # (batch_size, 24)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        loss_history.append(epoch_loss/len(train_loader))\n",
    "\n",
    "    # fin_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    return get_weights(model), loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae20e7b3",
   "metadata": {},
   "source": [
    "### FedAvg-diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9cb5b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment with model: transformer\n",
      "Round 1/50\n",
      "Sampled 211 clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training clients:  28%|██▊       | 60/211 [05:46<13:50,  5.50s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# difficulty_tracker = TimeSeriesDifficultyWeight(num_clients=NUM_CLIENTS)\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    print(f\"Starting experiment with model: {model_name}\")\n",
    "    difficulty_tracker = TimeSeriesDifficultyWeight(num_clients=NUM_CLIENTS)\n",
    "\n",
    "    model_dir = os.path.join(\"results\", model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    global_model = model_fn(model_name).to(DEVICE)\n",
    "    global_weights = get_weights(global_model)\n",
    "\n",
    "    for rnd in range(NUM_ROUNDS):\n",
    "        print(f\"Round {rnd+1}/{NUM_ROUNDS}\")\n",
    "\n",
    "        # === Difficulty-aware sampling ===\n",
    "        sampling_probs = difficulty_tracker.get_sampling_probabilities(min_prob=0.05)\n",
    "        sampled_clients = np.random.choice(\n",
    "            np.arange(NUM_CLIENTS),\n",
    "            size=int(CLIENT_FRAC * NUM_CLIENTS),\n",
    "            replace=False,\n",
    "            p=sampling_probs\n",
    "        )\n",
    "        print(f\"Sampled {len(sampled_clients)} clients\")\n",
    "\n",
    "        local_weights = []\n",
    "\n",
    "        for cid in tqdm(sampled_clients, desc=\"Training clients\"):\n",
    "            local_model = model_fn(model_name).to(DEVICE)\n",
    "            set_weights(local_model, global_weights)\n",
    "            train_loader, test_loader = load_energy_data_feather(cid, filepath=DATA_FILE)\n",
    "\n",
    "            updated_weights, loss_history = train_model_transformer(\n",
    "                local_model, train_loader,\n",
    "                device=DEVICE,\n",
    "                learning_rate=LR,\n",
    "                loss_fn=None,\n",
    "                optimizer_class=optim.Adam,\n",
    "                epochs=LOCAL_EPOCHS\n",
    "            )\n",
    "            # local_weights.append(updated_weights)\n",
    "\n",
    "            # === Update difficulty score ===\n",
    "            difficulty_tracker.update(cid, loss_history)\n",
    "\n",
    "            local_weights.append(updated_weights)\n",
    "\n",
    "        # === FedAvg-style aggregation ===\n",
    "        global_weights = average_weights(local_weights)\n",
    "        set_weights(global_model, global_weights)\n",
    "\n",
    "        checkpoint_path = os.path.join(model_dir, f\"{model_name}_round_{rnd+1}_fedAvg_diff.pt\")\n",
    "        torch.save(global_model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved global model to {checkpoint_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
