{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c26839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from Models import MoELSTM\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "import random\n",
    "from Models import MoELSTM, LSTMModel, train_model\n",
    "from Preprocess import (\n",
    "    compute_metrics,\n",
    "    convert_timeseries_to_numpy,\n",
    "    create_dataloader,\n",
    "    load_building_series,\n",
    "    split_series_list,\n",
    ")\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from Models import model_fn\n",
    "from tqdm import tqdm\n",
    "from my_utils import train_model, load_energy_data_feather, get_weights, set_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d0695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AggregationStrategy import sync_aggregate,average_weights,sync_aggregate_norm,sync_aggregate_softmax, fedavgm_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cf3f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(\"train_final.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a5334f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>primary_use</th>\n",
       "      <th>air_temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7593144</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-21 01:00:00</td>\n",
       "      <td>72.221012</td>\n",
       "      <td>Education</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7593145</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-21 01:00:00</td>\n",
       "      <td>39.611586</td>\n",
       "      <td>Education</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7593146</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-21 01:00:00</td>\n",
       "      <td>1.920567</td>\n",
       "      <td>Education</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7593147</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-21 01:00:00</td>\n",
       "      <td>111.532464</td>\n",
       "      <td>Education</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7593148</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-21 01:00:00</td>\n",
       "      <td>456.734799</td>\n",
       "      <td>Education</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         building_id  meter           timestamp  meter_reading primary_use  \\\n",
       "7593144            0      0 2016-05-21 01:00:00      72.221012   Education   \n",
       "7593145            1      0 2016-05-21 01:00:00      39.611586   Education   \n",
       "7593146            2      0 2016-05-21 01:00:00       1.920567   Education   \n",
       "7593147            3      0 2016-05-21 01:00:00     111.532464   Education   \n",
       "7593148            4      0 2016-05-21 01:00:00     456.734799   Education   \n",
       "\n",
       "         air_temperature  \n",
       "7593144             25.6  \n",
       "7593145             25.6  \n",
       "7593146             25.6  \n",
       "7593147             25.6  \n",
       "7593148             25.6  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1317d9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11712248 entries, 7593144 to 20216099\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Dtype         \n",
      "---  ------           -----         \n",
      " 0   building_id      int64         \n",
      " 1   meter            int64         \n",
      " 2   timestamp        datetime64[ns]\n",
      " 3   meter_reading    float64       \n",
      " 4   primary_use      object        \n",
      " 5   air_temperature  float64       \n",
      "dtypes: datetime64[ns](1), float64(2), int64(2), object(1)\n",
      "memory usage: 625.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97aecb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Config\n",
    "# List of models to experiment with\n",
    "MODEL_NAMES = [\"lstm\", \"gru\", \"moe_lstm\", \"moe_gru\"]\n",
    "\n",
    "# Config\n",
    "NUM_CLIENTS = 1410\n",
    "CLIENT_FRAC = 0.15\n",
    "NUM_ROUNDS = 50\n",
    "LOCAL_EPOCHS = 5\n",
    "LR = 0.001\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_FILE =\"train_final.feather\" # \"meter_0_data_cleaned.feather\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "647a873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_energy_data_feather(cid, filepath=\"meter_0_data_cleaned.feather\"):\n",
    "    \"\"\"Load, preprocess, and return train/test dataloaders for a client.\"\"\"\n",
    "    df = pd.read_feather(filepath)\n",
    "    df = df[df['building_id'] == cid]\n",
    "    df['meter_reading'] = df['meter_reading'].fillna(0)\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No data found for building_id {cid}\")\n",
    "\n",
    "    try:\n",
    "        ts = TimeSeries.from_dataframe(\n",
    "            df,\n",
    "            time_col='timestamp',\n",
    "            value_cols='meter_reading',\n",
    "            fill_missing_dates=True,\n",
    "            freq='h'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to construct TimeSeries: {e}\")\n",
    "\n",
    "    train_series, test_series = ts.split_before(0.75)\n",
    "\n",
    "    if len(train_series) == 0 or len(test_series) == 0:\n",
    "        raise ValueError(f\"Empty time series for building_id {cid}. Train: {len(train_series)}, Test: {len(test_series)}\")\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0.1, 1))\n",
    "    transformer = Scaler(scaler)\n",
    "    transformed_train_series = transformer.fit_transform(train_series)\n",
    "    transformed_test_series = transformer.transform(test_series)\n",
    "\n",
    "    X_train, y_train = convert_timeseries_to_numpy(transformed_train_series, input_len=168, output_len=24)\n",
    "    # X_test, y_test = convert_timeseries_to_numpy(transformed_test_series, input_len=168, output_len=24)\n",
    "\n",
    "    X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0)\n",
    "    # X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "    # y_test = np.nan_to_num(y_test, nan=0.0)\n",
    "\n",
    "\n",
    "    if len(X_train) == 0 :#or len(X_test) == 0:\n",
    "        raise ValueError(f\"Client {cid} has no data after preprocessing.\")\n",
    "\n",
    "    train_loader = create_dataloader(X_train, y_train, batch_size=512)\n",
    "    # test_loader = create_dataloader(X_test, y_test, batch_size=256)\n",
    "\n",
    "    return train_loader, 0 # test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e397eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model_transformer(model, train_loader, device=None, learning_rate=0.001, loss_fn=None, optimizer_class=optim.Adam, epochs=50):\n",
    "    \"\"\"Train the model and return the average loss.\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    loss_fn = loss_fn or nn.MSELoss()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    loss_history = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            if y_batch.dim() == 3 and y_batch.shape[-1] == 1:\n",
    "                y_batch = y_batch.squeeze(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            #loss = loss_fn(output, y_batch)\n",
    "            loss = loss_fn(output.squeeze(-1), y_batch)  # (batch_size, 24)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        loss_history.append(epoch_loss/len(train_loader))\n",
    "\n",
    "    # fin_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    return get_weights(model), loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a69607",
   "metadata": {},
   "source": [
    "### Diff-Sync FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c327259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for transformer model\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30bac448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    \"\"\"Transformer model for univariate time series forecasting\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        context_length: int = 168,\n",
    "        forecast_horizon: int = 24,\n",
    "        d_model: int = 128,\n",
    "        n_heads: int = 8,\n",
    "        n_layers: int = 6,\n",
    "        d_ff: int = 512,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.context_length = context_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input embedding and positional encoding\n",
    "        self.input_embedding = nn.Linear(1, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len=context_length + forecast_horizon)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Learnable forecast tokens\n",
    "        self.forecast_tokens = nn.Parameter(torch.randn(forecast_horizon, 1, d_model))\n",
    "        \n",
    "    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            src: Input tensor of shape (batch_size, context_length, 1)\n",
    "            src_mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            Forecasted values of shape (batch_size, forecast_horizon, 1)\n",
    "        \"\"\"\n",
    "        batch_size = src.size(0)\n",
    "        \n",
    "        # Embed input sequence\n",
    "        src_embedded = self.input_embedding(src)  # (batch_size, context_length, d_model)\n",
    "        src_embedded = src_embedded.transpose(0, 1)  # (context_length, batch_size, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        src_embedded = self.pos_encoding(src_embedded)\n",
    "        \n",
    "        # Prepare forecast tokens\n",
    "        forecast_tokens = self.forecast_tokens.expand(-1, batch_size, -1)  # (forecast_horizon, batch_size, d_model)\n",
    "        \n",
    "        # Concatenate input and forecast tokens\n",
    "        full_sequence = torch.cat([src_embedded, forecast_tokens], dim=0)  # (context_length + forecast_horizon, batch_size, d_model)\n",
    "        \n",
    "        # Create causal mask to prevent looking at future values\n",
    "        seq_len = full_sequence.size(0)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(src.device)\n",
    "        \n",
    "        # Apply transformer\n",
    "        transformer_output = self.transformer_encoder(full_sequence, mask=causal_mask)\n",
    "        \n",
    "        # Extract forecast part\n",
    "        forecast_output = transformer_output[self.context_length:]  # (forecast_horizon, batch_size, d_model)\n",
    "        \n",
    "        # Project to output dimension\n",
    "        forecast_output = self.dropout(forecast_output)\n",
    "        predictions = self.output_projection(forecast_output)  # (forecast_horizon, batch_size, 1)\n",
    "        \n",
    "        # Transpose back to (batch_size, forecast_horizon, 1)\n",
    "        predictions = predictions.transpose(0, 1)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29066235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully for building_id=1\n",
      "Training batches: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/flower/lib/python3.11/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully!\n",
      "Training on device: cuda\n",
      "Number of parameters: 1,193,089\n",
      "Epoch 0, Batch 0, Loss: 0.584771\n",
      "New best model saved with loss: 0.137521\n",
      "Epoch 0 completed. Average Loss: 0.137521, LR: 1.00e-04\n",
      "Epoch 1, Batch 0, Loss: 0.063717\n",
      "New best model saved with loss: 0.058602\n",
      "Epoch 1 completed. Average Loss: 0.058602, LR: 1.00e-04\n",
      "Epoch 2, Batch 0, Loss: 0.059831\n",
      "New best model saved with loss: 0.048380\n",
      "Epoch 2 completed. Average Loss: 0.048380, LR: 1.00e-04\n",
      "Epoch 3, Batch 0, Loss: 0.043081\n",
      "New best model saved with loss: 0.038962\n",
      "Epoch 3 completed. Average Loss: 0.038962, LR: 1.00e-04\n",
      "Epoch 4, Batch 0, Loss: 0.035663\n",
      "New best model saved with loss: 0.032616\n",
      "Epoch 4 completed. Average Loss: 0.032616, LR: 1.00e-04\n",
      "Epoch 5, Batch 0, Loss: 0.030419\n",
      "New best model saved with loss: 0.027737\n",
      "Epoch 5 completed. Average Loss: 0.027737, LR: 1.00e-04\n",
      "Epoch 6, Batch 0, Loss: 0.026169\n",
      "New best model saved with loss: 0.023609\n",
      "Epoch 6 completed. Average Loss: 0.023609, LR: 1.00e-04\n",
      "Epoch 7, Batch 0, Loss: 0.021561\n",
      "New best model saved with loss: 0.019951\n",
      "Epoch 7 completed. Average Loss: 0.019951, LR: 1.00e-04\n",
      "Epoch 8, Batch 0, Loss: 0.017818\n",
      "New best model saved with loss: 0.017571\n",
      "Epoch 8 completed. Average Loss: 0.017571, LR: 1.00e-04\n",
      "Epoch 9, Batch 0, Loss: 0.016915\n",
      "New best model saved with loss: 0.015304\n",
      "Epoch 9 completed. Average Loss: 0.015304, LR: 1.00e-04\n",
      "Epoch 10, Batch 0, Loss: 0.014673\n",
      "New best model saved with loss: 0.013848\n",
      "Epoch 10 completed. Average Loss: 0.013848, LR: 1.00e-04\n",
      "Epoch 11, Batch 0, Loss: 0.013160\n",
      "New best model saved with loss: 0.012585\n",
      "Epoch 11 completed. Average Loss: 0.012585, LR: 1.00e-04\n",
      "Epoch 12, Batch 0, Loss: 0.011851\n",
      "New best model saved with loss: 0.011590\n",
      "Epoch 12 completed. Average Loss: 0.011590, LR: 1.00e-04\n",
      "Epoch 13, Batch 0, Loss: 0.010291\n",
      "New best model saved with loss: 0.010734\n",
      "Epoch 13 completed. Average Loss: 0.010734, LR: 1.00e-04\n",
      "Epoch 14, Batch 0, Loss: 0.010582\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  \n",
    "    train_loader, _ = load_energy_data_feather(cid=1, filepath=DATA_FILE)\n",
    "    \n",
    "    print(f\"Data loaded successfully for building_id=1\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = TimeSeriesTransformer(\n",
    "        context_length=168,\n",
    "        forecast_horizon=24,\n",
    "        d_model=128,\n",
    "        n_heads=8,\n",
    "        n_layers=6,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Setup training\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    print(f\"Model created successfully!\")\n",
    "    print(f\"Training on device: {device}\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Training loop with improvements\n",
    "    num_epochs = 50\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_losses = []\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, (context, target) in enumerate(train_loader):\n",
    "            context, target = context.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(context)\n",
    "            loss = criterion(predictions, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        avg_loss = np.mean(train_losses)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), f'best_model_building_{1}.pth')\n",
    "            print(f\"New best model saved with loss: {best_loss:.6f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.6f}, LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # Load best model for inference\n",
    "    model.load_state_dict(torch.load(f'best_model_building_{1}.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    # Example inference and evaluation\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        \n",
    "        for context, target in train_loader:\n",
    "            context = context.to(device)\n",
    "            forecast = model(context)\n",
    "            \n",
    "            all_predictions.append(forecast.cpu())\n",
    "            all_targets.append(target)\n",
    "            \n",
    "            if len(all_predictions) == 1:  # Print shapes for first batch\n",
    "                print(f\"Input shape: {context.shape}\")  # Should be (batch_size, 168, 1)\n",
    "                print(f\"Forecast shape: {forecast.shape}\")  # Should be (batch_size, 24)\n",
    "                print(f\"Target shape: {target.shape}\")  # Should be (batch_size, 24)\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        # all_predictions = torch.cat(all_predictions, dim=0)\n",
    "        # all_targets = torch.cat(all_targets, dim=0)\n",
    "        \n",
    "        # mse = nn.MSELoss()(all_predictions, all_targets)\n",
    "        # mae = nn.L1Loss()(all_predictions, all_targets)\n",
    "        \n",
    "        # print(f\"\\nFinal Model Performance:\")\n",
    "        # print(f\"MSE: {mse.item():.6f}\")\n",
    "        # print(f\"MAE: {mae.item():.6f}\")\n",
    "        # print(f\"RMSE: {torch.sqrt(mse).item():.6f}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error during execution: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65a8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0e8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b006b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41e8cc8f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
