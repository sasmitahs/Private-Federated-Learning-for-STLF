{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "244cb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from typing import List\n",
    "# Config\n",
    "NUM_CLIENTS = 10\n",
    "NUM_EPOCHS = 5\n",
    "LOCAL_EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.01\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=False, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "573bd92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into clients\n",
    "def split_dataset(dataset, num_clients):\n",
    "    data_len = len(dataset)\n",
    "    indices = np.arange(data_len)\n",
    "    np.random.shuffle(indices)\n",
    "    split_size = data_len // num_clients\n",
    "    return [Subset(dataset, indices[i*split_size:(i+1)*split_size]) for i in range(num_clients)]\n",
    "\n",
    "client_datasets = split_dataset(train_dataset, NUM_CLIENTS)\n",
    "\n",
    "# Local training\n",
    "def train_local(model, train_data, epochs=1):\n",
    "    model = model.to(DEVICE)\n",
    "    model.train()\n",
    "    loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.state_dict()\n",
    "\n",
    "# Model aggregation (FedAvg)\n",
    "def average_models(state_dicts):\n",
    "    avg_model = {}\n",
    "    for key in state_dicts[0]:\n",
    "        avg_model[key] = sum([sd[key] for sd in state_dicts]) / len(state_dicts)\n",
    "    return avg_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a335fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1, Test Accuracy: 0.7448\n",
      "Round 2, Test Accuracy: 0.8265\n",
      "Round 3, Test Accuracy: 0.8595\n",
      "Round 4, Test Accuracy: 0.8759\n",
      "Round 5, Test Accuracy: 0.8851\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Test\n",
    "def test(model, test_data):\n",
    "    model.eval()\n",
    "    loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            preds = model(x).argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Federated training\n",
    "global_model = MLP().to(DEVICE)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    local_weights = []\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        local_model = MLP().to(DEVICE)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        local_weights.append(train_local(local_model, client_datasets[i], epochs=LOCAL_EPOCHS))\n",
    "\n",
    "    averaged_weights = average_models(local_weights)\n",
    "    global_model.load_state_dict(averaged_weights)\n",
    "\n",
    "    acc = test(global_model, test_dataset)\n",
    "    print(f\"Round {epoch+1}, Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a6693f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AggregationStrategy import sync_aggregate,average_weights,sync_aggregate_norm,sync_aggregate_softmax, fedavgm_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1c412a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(model):\n",
    "    return [p.detach().cpu().numpy() for p in model.parameters()]\n",
    "\n",
    "\n",
    "def set_weights(model, weights):\n",
    "    params = list(model.parameters())\n",
    "    if len(weights) != len(params):\n",
    "        raise ValueError(f\"Mismatch in weights ({len(weights)}) and model parameters ({len(params)})\")\n",
    "\n",
    "    for p, w in zip(params, weights):\n",
    "        p.data = torch.tensor(w, dtype=p.dtype, device=p.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6a1fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local(model, train_data, epochs=1):\n",
    "    model = model.to(DEVICE)\n",
    "    model.train()\n",
    "    loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return get_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7b81983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC Weights:[ -86.81517  -142.6893   -364.6838     -9.192195   -4.08542  -110.847046\n",
      "   36.528458  202.88916   167.32198   311.63342 ] \n",
      "Round 1, Test Accuracy: 0.2338\n",
      "SYNC Weights:[-1.3198408  -1.8052368   3.354141   -0.20724773 -1.8543218   1.8870451\n",
      "  0.18543218 -0.583566   -0.19633995  0.5890199 ] \n",
      "Round 2, Test Accuracy: 0.1031\n",
      "SYNC Weights:[-2.7790623  -0.0652362  -2.870393   -0.96549577  4.0968337  -4.122928\n",
      "  2.8051567  -0.24789758  1.4743382   2.7268732 ] \n",
      "Round 3, Test Accuracy: 0.1839\n",
      "SYNC Weights:[ -4.0026927  -8.267124  -11.656964    0.6575396   3.2110918 -12.601779\n",
      "  14.804217    3.823944    5.3433075   8.7395315] \n",
      "Round 4, Test Accuracy: 0.2173\n",
      "SYNC Weights:[ -1.200724   17.819838  -14.1357975  -1.8283752   5.212234  -44.017452\n",
      "   6.221934    4.475426    1.5554835  25.952015 ] \n",
      "Round 5, Test Accuracy: 0.2118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Test\n",
    "def test(model, test_data):\n",
    "    model.eval()\n",
    "    loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            preds = model(x).argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Federated training\n",
    "global_model = MLP().to(DEVICE)\n",
    "global_weights = get_weights(global_model)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    local_weights = []\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        local_model = MLP().to(DEVICE)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        local_weights.append(train_local(local_model, client_datasets[i], epochs=LOCAL_EPOCHS))\n",
    "\n",
    "    sync_weights = sync_aggregate(global_weights,local_weights)\n",
    "    set_weights(global_model,sync_weights)\n",
    "    # global_model.load_state_dict(averaged_weights)\n",
    "\n",
    "    acc = test(global_model, test_dataset)\n",
    "    print(f\"Round {epoch+1}, Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdd9d5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC Weights:[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] \n",
      "Round 1, Test Accuracy: 0.7071\n",
      "SYNC Weights:[5.324934164434305e-44, 0.0, 5.692147580394696e-27, 1.3877241161447416e-34, 0.0, 1.0, 0.0, 2.1498160520285614e-40, 0.0, 0.0] \n",
      "Round 2, Test Accuracy: 0.8275\n",
      "SYNC Weights:[0.3264521062374115, 0.031513601541519165, 0.08028356730937958, 0.0024079345166683197, 0.20452912151813507, 0.001194120617583394, 0.000317467754939571, 0.0019059550249949098, 0.3264521062374115, 0.024943992495536804] \n",
      "Round 3, Test Accuracy: 0.8587\n",
      "SYNC Weights:[0.4740835726261139, 0.0017221422167494893, 0.4740835726261139, 0.0001965281117008999, 0.007014804519712925, 2.895207580877468e-05, 4.846006504521938e-06, 0.0005457857041619718, 0.03688597306609154, 0.005433955695480108] \n",
      "Round 4, Test Accuracy: 0.8667\n",
      "SYNC Weights:[0.0021372162736952305, 0.18804344534873962, 0.0009546735091134906, 0.1437452882528305, 0.020047184079885483, 0.42097073793411255, 0.1572105884552002, 0.05368135869503021, 0.007486575283110142, 0.005722932983189821] \n",
      "Round 5, Test Accuracy: 0.8698\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global_model = MLP().to(DEVICE)\n",
    "global_weights = get_weights(global_model)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    local_weights = []\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        local_model = MLP().to(DEVICE)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        local_weights.append(train_local(local_model, client_datasets[i], epochs=LOCAL_EPOCHS))\n",
    "\n",
    "    sync_weights = sync_aggregate_softmax(global_weights,local_weights)\n",
    "    set_weights(global_model,sync_weights)\n",
    "    # global_model.load_state_dict(averaged_weights)\n",
    "\n",
    "    acc = test(global_model, test_dataset)\n",
    "    print(f\"Round {epoch+1}, Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30767af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC Weights:[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] \n",
      "Round 1, Test Accuracy: 0.6989\n",
      "SYNC Weights:[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0] \n",
      "Round 2, Test Accuracy: 0.8251\n",
      "SYNC Weights:[0.6339377164840698, 0.04061416909098625, 0.02072162553668022, 0.0012551635736599565, 0.16502106189727783, 0.06727717071771622, 0.00038659514393657446, 0.02072162553668022, 0.04061416909098625, 0.00945064052939415] \n",
      "Round 3, Test Accuracy: 0.8528\n",
      "SYNC Weights:[4.014902515336871e-06, 0.0032507923897355795, 4.9464390031062067e-05, 0.004940362647175789, 0.00022949933190830052, 5.6869950640248135e-05, 0.9912301301956177, 4.9464390031062067e-05, 7.517306949011981e-05, 0.00011424360855016857] \n",
      "Round 4, Test Accuracy: 0.8543\n",
      "SYNC Weights:[0.0, 0.5, 0.0, 5.701456694999843e-14, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0] \n",
      "Round 5, Test Accuracy: 0.8588\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global_model = MLP().to(DEVICE)\n",
    "global_weights = get_weights(global_model)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    local_weights = []\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        local_model = MLP().to(DEVICE)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        local_weights.append(train_local(local_model, client_datasets[i], epochs=LOCAL_EPOCHS))\n",
    "\n",
    "    sync_weights = sync_aggregate_softmax(global_weights,local_weights)\n",
    "    set_weights(global_model,sync_weights)\n",
    "    # global_model.load_state_dict(averaged_weights)\n",
    "\n",
    "    acc = test(global_model, test_dataset)\n",
    "    print(f\"Round {epoch+1}, Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be3aeb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC Weights:[np.float32(0.2745864), np.float32(0.57365245), np.float32(1.0), np.float32(0.611318), np.float32(0.094377644), np.float32(0.4010134), np.float32(0.33258045), np.float32(0.035223752), np.float32(0.3645687), np.float32(0.0)] \n",
      "Round 1, Test Accuracy: 0.7613\n",
      "SYNC Weights:[np.float32(0.8306452), np.float32(0.8709678), np.float32(0.5806452), np.float32(0.7741936), np.float32(0.8225807), np.float32(0.9758065), np.float32(0.0), np.float32(1.0), np.float32(0.6048387), np.float32(0.7741936)] \n",
      "Round 2, Test Accuracy: 0.8621\n",
      "SYNC Weights:[np.float32(0.58227843), np.float32(0.7341772), np.float32(0.59493667), np.float32(1.0), np.float32(0.6835443), np.float32(0.75949365), np.float32(0.0), np.float32(0.59493667), np.float32(0.58227843), np.float32(0.35443035)] \n",
      "Round 3, Test Accuracy: 0.8779\n",
      "SYNC Weights:[np.float32(0.6724138), np.float32(0.6206897), np.float32(0.7413793), np.float32(0.87931037), np.float32(0.44827586), np.float32(0.8448276), np.float32(0.0), np.float32(1.0), np.float32(0.6724138), np.float32(0.6896552)] \n",
      "Round 4, Test Accuracy: 0.8862\n",
      "SYNC Weights:[np.float32(0.78378373), np.float32(0.8108108), np.float32(0.7432432), np.float32(0.7702702), np.float32(0.8378378), np.float32(0.7027027), np.float32(0.0), np.float32(1.0), np.float32(0.6621622), np.float32(0.7567567)] \n",
      "Round 5, Test Accuracy: 0.8923\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global_model = MLP().to(DEVICE)\n",
    "global_weights = get_weights(global_model)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    local_weights = []\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        local_model = MLP().to(DEVICE)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        local_weights.append(train_local(local_model, client_datasets[i], epochs=LOCAL_EPOCHS))\n",
    "\n",
    "    sync_weights = sync_aggregate_norm(global_weights,local_weights)\n",
    "    set_weights(global_model,sync_weights)\n",
    "    # global_model.load_state_dict(averaged_weights)\n",
    "\n",
    "    acc = test(global_model, test_dataset)\n",
    "    print(f\"Round {epoch+1}, Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9371dc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC Weights:[np.float32(0.43710375), np.float32(0.4477811), np.float32(0.90397066), np.float32(1.0), np.float32(0.35542205), np.float32(0.6184184), np.float32(0.63610274), np.float32(0.5550217), np.float32(0.17464131), np.float32(0.0)] \n",
      "Round 1, Test Accuracy: 0.7568\n",
      "SYNC Weights:[np.float32(0.88235295), np.float32(0.14285715), np.float32(0.56302524), np.float32(0.45378152), np.float32(0.2857143), np.float32(0.62184876), np.float32(0.0), np.float32(0.5378151), np.float32(1.0), np.float32(0.29411766)] \n",
      "Round 2, Test Accuracy: 0.8597\n",
      "SYNC Weights:[np.float32(0.9866666), np.float32(0.44), np.float32(0.9333333), np.float32(0.61333334), np.float32(0.013333332), np.float32(0.97333336), np.float32(0.0), np.float32(0.8666667), np.float32(1.0), np.float32(0.61333334)] \n",
      "Round 3, Test Accuracy: 0.8775\n",
      "SYNC Weights:[np.float32(0.18279569), np.float32(0.46236557), np.float32(0.19354837), np.float32(0.32258064), np.float32(1.0), np.float32(0.0), np.float32(0.8172043), np.float32(0.13978493), np.float32(0.032258056), np.float32(0.22580642)] \n",
      "Round 4, Test Accuracy: 0.8871\n",
      "SYNC Weights:[np.float32(0.058823533), np.float32(0.5), np.float32(0.13235293), np.float32(0.29411763), np.float32(1.0), np.float32(0.058823533), np.float32(0.82352936), np.float32(0.0882353), np.float32(0.0), np.float32(0.6764706)] \n",
      "Round 5, Test Accuracy: 0.8934\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global_model = MLP().to(DEVICE)\n",
    "global_weights = get_weights(global_model)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    local_weights = []\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        local_model = MLP().to(DEVICE)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        local_weights.append(train_local(local_model, client_datasets[i], epochs=LOCAL_EPOCHS))\n",
    "\n",
    "    sync_weights = sync_aggregate_norm(global_weights,local_weights)\n",
    "    set_weights(global_model,sync_weights)\n",
    "    # global_model.load_state_dict(averaged_weights)\n",
    "\n",
    "    acc = test(global_model, test_dataset)\n",
    "    print(f\"Round {epoch+1}, Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25ef1b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDifficultyWeight:\n",
    "    def __init__(self, num_clients, accumulate_iters=20):\n",
    "        self.num_clients = num_clients\n",
    "        self.last_loss = torch.ones(num_clients).float().to(DEVICE)\n",
    "        self.learn_score = torch.zeros(num_clients).float().to(DEVICE)\n",
    "        self.unlearn_score = torch.zeros(num_clients).float().to(DEVICE)\n",
    "        self.ema_difficulty = torch.ones(num_clients).float().to(DEVICE)\n",
    "        self.accumulate_iters = accumulate_iters\n",
    "\n",
    "    def update(self, cid: int, loss_history: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        Update difficulty based on loss trend for a client.\n",
    "        Expects a list of per-epoch losses.\n",
    "        \"\"\"\n",
    "        current_loss = torch.tensor(loss_history[-1], dtype=torch.float32).to(DEVICE)\n",
    "        previous_loss = self.last_loss[cid]\n",
    "        delta = current_loss - previous_loss\n",
    "        ratio = torch.log((current_loss + 1e-8) / (previous_loss + 1e-8))\n",
    "\n",
    "        learn = torch.where(delta < 0, -delta * ratio, torch.tensor(0.0, device=current_loss.device))\n",
    "        unlearn = torch.where(delta >= 0, delta * ratio, torch.tensor(0.0, device=current_loss.device))\n",
    "\n",
    "        # EMA update\n",
    "        momentum = (self.accumulate_iters - 1) / self.accumulate_iters\n",
    "        self.learn_score[cid] = momentum * self.learn_score[cid] + (1 - momentum) * learn\n",
    "        self.unlearn_score[cid] = momentum * self.unlearn_score[cid] + (1 - momentum) * unlearn\n",
    "\n",
    "        # Difficulty score\n",
    "        diff_ratio = (self.unlearn_score[cid] + 1e-8) / (self.learn_score[cid] + 1e-8)\n",
    "        difficulty = diff_ratio #torch.pow(diff_ratio, 1 / 5)\n",
    "\n",
    "        # Smooth difficulty over rounds\n",
    "        self.ema_difficulty[cid] = momentum * self.ema_difficulty[cid] + (1 - momentum) * difficulty\n",
    "\n",
    "        self.last_loss[cid] = current_loss\n",
    "        return self.ema_difficulty[cid].item()\n",
    "\n",
    "    def get_normalized_weights(self, client_ids: List[int]) -> List[float]:\n",
    "        weights = [self.ema_difficulty[cid].item() for cid in client_ids]\n",
    "        total = sum(weights)\n",
    "        if total == 0:\n",
    "            return [1.0 / len(client_ids)] * len(client_ids)\n",
    "        return [w / total for w in weights]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5908746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, device, learning_rate, loss_fn, optimizer_class, epochs):\n",
    "    model.train()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss() if loss_fn is None else loss_fn\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        loss_history.append(running_loss / len(dataloader))\n",
    "    \n",
    "    return get_weights(model), loss_history\n",
    "\n",
    "def train_local2(model, train_data, epochs=1):\n",
    "    model = model.to(DEVICE)\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0 \n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        loss_history.append(running_loss / len(loader))\n",
    "    return get_weights(model), loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c99a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# difficulty_tracker = TimeSeriesDifficultyWeight(num_clients=NUM_CLIENTS)\n",
    "\n",
    "# for rnd in range(NUM_ROUNDS):\n",
    "#     sampled_clients = random.sample(range(NUM_CLIENTS), int(CLIENT_FRAC * NUM_CLIENTS))\n",
    "#     local_weights = []\n",
    "#     difficulty_scores = []\n",
    "\n",
    "#     for cid in tqdm(sampled_clients):\n",
    "#         model = model_fn(model_name).to(DEVICE)\n",
    "#         set_weights(model, global_weights)\n",
    "#         train_loader, _ = load_energy_data_feather(cid, filepath=DATA_FILE)\n",
    "#         updated_weights, loss_history = train_model(\n",
    "#             model, train_loader,\n",
    "#             device=DEVICE, learning_rate=LR,\n",
    "#             loss_fn=None, optimizer_class=optim.Adam,\n",
    "#             epochs=LOCAL_EPOCHS\n",
    "#         )\n",
    "#         local_weights.append(updated_weights)\n",
    "\n",
    "#         # Update difficulty\n",
    "#         difficulty = difficulty_tracker.update(cid, loss_history)\n",
    "#         difficulty_scores.append(difficulty)\n",
    "\n",
    "#     # Normalize difficulty scores\n",
    "#     normalized_weights = difficulty_tracker.get_normalized_weights(sampled_clients)\n",
    "\n",
    "#     # Difficulty-aware weighted aggregation\n",
    "#     global_weights = average_weights(local_weights, client_weights=normalized_weights)\n",
    "#     set_weights(global_model, global_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7314b5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1, Test Accuracy: 0.7628\n",
      "Round 2, Test Accuracy: 0.8346\n",
      "Round 3, Test Accuracy: 0.8635\n",
      "Round 4, Test Accuracy: 0.8796\n",
      "Round 5, Test Accuracy: 0.8874\n"
     ]
    }
   ],
   "source": [
    "\n",
    "difficulty_tracker = TimeSeriesDifficultyWeight(num_clients=NUM_CLIENTS)\n",
    "global_model = MLP().to(DEVICE)\n",
    "global_weights = get_weights(global_model)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    local_weights = []\n",
    "    local_weights = []\n",
    "    difficulty_scores = []\n",
    "    for cid in range(NUM_CLIENTS):\n",
    "        local_model = MLP().to(DEVICE)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        updated_weights, loss_history = train_local2(local_model, client_datasets[cid], epochs=LOCAL_EPOCHS)\n",
    "        local_weights.append(updated_weights)\n",
    "        difficulty = difficulty_tracker.update(cid, loss_history)\n",
    "        difficulty_scores.append(difficulty)\n",
    "\n",
    "    # Normalize difficulty scores\n",
    "    normalized_weights = difficulty_tracker.get_normalized_weights(range(NUM_CLIENTS))\n",
    "    global_weights = average_weights(local_weights, client_weights=normalized_weights)\n",
    "\n",
    "    # sync_weights = sync_aggregate_norm(global_weights,local_weights)\n",
    "    set_weights(global_model,global_weights)\n",
    "    # global_model.load_state_dict(averaged_weights)\n",
    "\n",
    "    acc = test(global_model, test_dataset)\n",
    "    print(f\"Round {epoch+1}, Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06dd808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_weights [0.09928567285029549, 0.10047606193106842, 0.10306820453686075, 0.1009480691539008, 0.09907237475317177, 0.10058047066814692, 0.10317079035889194, 0.09695239269951486, 0.09875751302937048, 0.09768845001877859]\n",
      "Round 1, Test Accuracy: 0.7486\n",
      "normalized_weights [0.09928567342350057, 0.10047606528495681, 0.10306820144287422, 0.10094806868653755, 0.09907237522979231, 0.10058047272431864, 0.10317078865635229, 0.09695239490616511, 0.09875751201842248, 0.09768844762708001]\n",
      "Round 2, Test Accuracy: 0.8294\n",
      "normalized_weights [0.09928567267514167, 0.10047606405340082, 0.10306820451792226, 0.10094806763763792, 0.09907237062346645, 0.10058047342087864, 0.1031707922430277, 0.09695239419872785, 0.09875750964989091, 0.09768845097990579]\n",
      "Round 3, Test Accuracy: 0.8586\n",
      "normalized_weights [0.09928567049219009, 0.10047606236244967, 0.10306821035629402, 0.1009480701159033, 0.09907236884912164, 0.10058047276661411, 0.10317079265936736, 0.0969523975093752, 0.09875750824217758, 0.09768844664650701]\n",
      "Round 4, Test Accuracy: 0.8746\n",
      "normalized_weights [0.09928566709660179, 0.10047606680490868, 0.1030682169709705, 0.1009480684885194, 0.09907237111288202, 0.10058047150240682, 0.10317079147493341, 0.09695240042066004, 0.09875750461708152, 0.09768844151103581]\n",
      "Round 5, Test Accuracy: 0.8854\n"
     ]
    }
   ],
   "source": [
    "\n",
    "difficulty_tracker = TimeSeriesDifficultyWeight(num_clients=NUM_CLIENTS)\n",
    "global_model = MLP().to(DEVICE)\n",
    "global_weights = get_weights(global_model)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    local_weights = []\n",
    "    local_weights = []\n",
    "    difficulty_scores = []\n",
    "    for cid in range(NUM_CLIENTS):\n",
    "        local_model = MLP().to(DEVICE)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        updated_weights, loss_history = train_local2(local_model, client_datasets[cid], epochs=LOCAL_EPOCHS)\n",
    "        local_weights.append(updated_weights)\n",
    "        difficulty = difficulty_tracker.update(cid, loss_history)\n",
    "        difficulty_scores.append(difficulty)\n",
    "\n",
    "    # Normalize difficulty scores\n",
    "    normalized_weights = difficulty_tracker.get_normalized_weights(range(NUM_CLIENTS))\n",
    "    global_weights = average_weights(local_weights, client_weights=normalized_weights)\n",
    "    print(f\"normalized_weights {normalized_weights}\")\n",
    "\n",
    "    # sync_weights = sync_aggregate_norm(global_weights,local_weights)\n",
    "    set_weights(global_model,global_weights)\n",
    "    # global_model.load_state_dict(averaged_weights)\n",
    "\n",
    "    acc = test(global_model, test_dataset)\n",
    "    print(f\"Round {epoch+1}, Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d94ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
