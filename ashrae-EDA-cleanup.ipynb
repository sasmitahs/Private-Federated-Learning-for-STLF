{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from darts import TimeSeries\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import csv\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute MAE, MSE, and RMSE between true and predicted values.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true (np.ndarray): True values\n",
    "    - y_pred (np.ndarray): Predicted values\n",
    "    \n",
    "    Returns:\n",
    "    - dict: {'MAE': ..., 'MSE': ..., 'RMSE': ...}\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse}\n",
    "\n",
    "\n",
    "def load_building_series(folder_path):\n",
    "    all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    series_list = []\n",
    "\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file, parse_dates=['timestamp'])\n",
    "        df = df.sort_values('timestamp')\n",
    "        ts = TimeSeries.from_dataframe(df, 'timestamp', 'kWh')\n",
    "        series_list.append(ts)\n",
    "\n",
    "    return series_list\n",
    "\n",
    "\n",
    "def split_series_list(series_list, train_ratio=0.75):\n",
    "    train_series = []\n",
    "    test_series = []\n",
    "    for ts in series_list:\n",
    "        train, test = ts.split_before(train_ratio)\n",
    "        train_series.append(train)\n",
    "        test_series.append(test)\n",
    "    return train_series, test_series\n",
    "\n",
    "\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def convert_timeseries_to_numpy(\n",
    "    series: TimeSeries,\n",
    "    input_len: int,\n",
    "    output_len: int,\n",
    "    drop_incomplete: bool = True\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts a single Darts TimeSeries into input-output pairs for supervised learning.\n",
    "\n",
    "    Args:\n",
    "        series (TimeSeries): The input time series.\n",
    "        input_len (int): Length of input window.\n",
    "        output_len (int): Length of output window.\n",
    "        drop_incomplete (bool): If True, drops windows that can't fit input+output.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: Arrays (X, y) where:\n",
    "            - X shape: (num_samples, input_len, num_features)\n",
    "            - y shape: (num_samples, output_len, num_features)\n",
    "    \"\"\"\n",
    "    values = series.values()  # shape: (T, D)\n",
    "    T = len(values)\n",
    "    max_i = T - input_len - output_len + 1\n",
    "\n",
    "    if drop_incomplete and max_i <= 0:\n",
    "        return np.empty((0, input_len, values.shape[1])), np.empty((0, output_len, values.shape[1]))\n",
    "\n",
    "    X_all, y_all = [], []\n",
    "\n",
    "    for i in range(max_i):\n",
    "        x_i = values[i : i + input_len]\n",
    "        y_i = values[i + input_len : i + input_len + output_len]\n",
    "        X_all.append(x_i)\n",
    "        y_all.append(y_i)\n",
    "\n",
    "    return np.array(X_all), np.array(y_all)\n",
    "\n",
    "\n",
    "def create_dataloader(X, y, batch_size=32):\n",
    "    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(\"train.csv\", usecols=[\"building_id\", \"timestamp\", \"meter\", \"meter_reading\"])\n",
    "# df[df['meter'] == 0].to_feather(\"meter_0_data.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_energy_data_feather(cid, filepath=\"meter_0_data.feather\"):\n",
    "    \"\"\"Load, preprocess, and return train/test dataloaders for a client.\"\"\"\n",
    "    df = pd.read_feather(filepath)\n",
    "    df = df[df['building_id'] == cid]\n",
    "    df['meter_reading'] = df['meter_reading'].fillna(0)\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No data found for building_id {cid}\")\n",
    "\n",
    "    try:\n",
    "        ts = TimeSeries.from_dataframe(\n",
    "            df,\n",
    "            time_col='timestamp',\n",
    "            value_cols='meter_reading',\n",
    "            fill_missing_dates=True,\n",
    "            freq='h'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to construct TimeSeries: {e}\")\n",
    "\n",
    "    train_series, test_series = ts.split_before(0.75)\n",
    "\n",
    "    if len(train_series) == 0 or len(test_series) == 0:\n",
    "        raise ValueError(f\"Empty time series for building_id {cid}. Train: {len(train_series)}, Test: {len(test_series)}\")\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0.1, 1))\n",
    "    transformer = Scaler(scaler)\n",
    "    transformed_train_series = transformer.fit_transform(train_series)\n",
    "    transformed_test_series = transformer.transform(test_series)\n",
    "\n",
    "    X_train, y_train = convert_timeseries_to_numpy(transformed_train_series, input_len=24, output_len=8)\n",
    "    X_test, y_test = convert_timeseries_to_numpy(transformed_test_series, input_len=24, output_len=8)\n",
    "\n",
    "    X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0)\n",
    "    X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "    y_test = np.nan_to_num(y_test, nan=0.0)\n",
    "\n",
    "\n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        raise ValueError(f\"Client {cid} has no data after preprocessing.\")\n",
    "\n",
    "    train_loader = create_dataloader(X_train, y_train, batch_size=512)\n",
    "    test_loader = create_dataloader(X_test, y_test, batch_size=256)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"meter_0_data.feather\"\n",
    "cid = 50\n",
    "# Load and filter by building\n",
    "df = pd.read_feather(filepath)\n",
    "# df = df[df['building_id'] == cid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0              0\n",
       "1              1\n",
       "2              2\n",
       "3              3\n",
       "4              4\n",
       "            ... \n",
       "20216095    1444\n",
       "20216096    1445\n",
       "20216097    1446\n",
       "20216098    1447\n",
       "20216099    1448\n",
       "Name: building_id, Length: 12060910, dtype: int64>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.building_id.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = load_energy_data_feather(0, filepath=\"meter_0_data.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, device = None, learning_rate=0.001, loss_fn=None, optimizer_class=optim.Adam, epochs=50):\n",
    "      \n",
    "        if device == None: \n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        loss_fn = loss_fn if loss_fn is not None else nn.MSELoss()\n",
    "        optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            epoch_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_batch = y_batch.squeeze(-1) if y_batch.dim() == 3 and y_batch.shape[-1] == 1 else y_batch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_batch)\n",
    "                loss = loss_fn(output, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        return avg_loss\n",
    "            # print(f\"[Epoch {epoch + 1}] Training Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\srina\\OneDrive\\Documents\\Federated Learning - Priyanka\\Diffusion_FL\\energy-ts-diffusion\\darts\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Models import MoELSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MoELSTM(\n",
    "            input_size=1, hidden_size=64, output_size=8,\n",
    "            num_experts=5, ffn_hidden_size=32\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in file 751\n",
      "Error in file 752\n",
      "Error in file 754\n",
      "Error in file 757\n",
      "Error in file 763\n",
      "Error in file 772\n",
      "Error in file 783\n",
      "Error in file 786\n",
      "Error in file 789\n",
      "Error in file 790\n",
      "Error in file 792\n",
      "Error in file 933\n",
      "Error in file 934\n",
      "Error in file 1072\n",
      "Error in file 1077\n",
      "Error in file 1078\n",
      "Error in file 1085\n",
      "Error in file 1112\n",
      "Error in file 1116\n",
      "Error in file 1132\n",
      "Error in file 1145\n",
      "Error in file 1155\n",
      "Error in file 1180\n",
      "Error in file 1187\n",
      "Error in file 1192\n",
      "Error in file 1204\n",
      "Error in file 1330\n",
      "Error in file 1349\n",
      "Error in file 1354\n",
      "Error in file 1372\n",
      "Error in file 1373\n",
      "Error in file 1374\n",
      "Error in file 1385\n",
      "Error in file 1388\n",
      "Error in file 1397\n",
      "Error in file 1426\n"
     ]
    }
   ],
   "source": [
    "exo = []\n",
    "for i in range(700,1448):\n",
    "    try:\n",
    "        train_dl, test_dl = load_energy_data_feather(i, filepath=\"meter_0_data.feather\")\n",
    "    except:\n",
    "        print(f\"Error in file {i}\")\n",
    "        exo.append(i)\n",
    "        \n",
    "    # train(net, train_dl, device = None, learning_rate=0.001, loss_fn=None, optimizer_class=optim.Adam, epochs=5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved as meter_0_data_cleaned.feather\n",
      "Removed 36 buildings, now have 1413 total buildings.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_feather(\"meter_0_data.feather\")\n",
    "\n",
    "# List of building IDs to exclude (the ones without data)\n",
    "excluded_building_ids = exo # [12, 25, 37, 51]  # Example values, replace with yours\n",
    "\n",
    "# Filter out the excluded buildings\n",
    "df_filtered = df[~df['building_id'].isin(excluded_building_ids)].copy()\n",
    "\n",
    "# Map old building IDs to new consecutive IDs (starting from 0)\n",
    "unique_ids = sorted(df_filtered['building_id'].unique())\n",
    "new_id_map = {old_id: new_id for new_id, old_id in enumerate(unique_ids)}\n",
    "df_filtered['building_id'] = df_filtered['building_id'].map(new_id_map)\n",
    "\n",
    "# Save the cleaned and updated data back to feather\n",
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "df_filtered.to_feather(\"meter_0_data_cleaned.feather\")\n",
    "\n",
    "print(\"Cleaned dataset saved as meter_0_data_cleaned.feather\")\n",
    "print(f\"Removed {len(excluded_building_ids)} buildings, now have {len(unique_ids)} total buildings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  1.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.009053892312714687"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(net, train_dl, device = None, learning_rate=0.001, loss_fn=None, optimizer_class=optim.Adam, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2, ..., 591, 783, 403])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['building_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df['building_id'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_energy_data(cid):\n",
    "\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "    df_ele = df[df.meter == 0]\n",
    "\n",
    "    temp_df = df_ele[df_ele['building_id']==cid]\n",
    "    # df = pd.read_csv(f\"Dataset/buildings/building_{cid}.csv\")\n",
    "\n",
    "    ts = TimeSeries.from_dataframe(temp_df, 'timestamp', 'meter_reading')\n",
    "    train_series, test_series = ts.split_before(0.75) \n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0.1, 1))\n",
    "    transformer = Scaler(scaler)\n",
    "    transformed_train_series = transformer.fit_transform(train_series)\n",
    "    transformed_test_series = transformer.fit_transform(test_series)\n",
    "\n",
    "\n",
    "    X_train, y_train = convert_timeseries_to_numpy(transformed_train_series, input_len=24, output_len=8)\n",
    "    X_test, y_test = convert_timeseries_to_numpy(transformed_test_series, input_len=24, output_len=8)\n",
    "\n",
    "    train_loader = create_dataloader(X_train, y_train, batch_size=512)\n",
    "    test_loader = create_dataloader(X_test, y_test, batch_size=256)\n",
    "\n",
    "   \n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = load_energy_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_values():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           0\n",
       "           ..\n",
       "20216095    0\n",
       "20216096    0\n",
       "20216097    0\n",
       "20216098    0\n",
       "20216099    0\n",
       "Name: meter, Length: 20216100, dtype: int64>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.meter.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_is_bad_zero(Xy_subset, min_interval=48, summer_start=3000, summer_end=7500):\n",
    "    \"\"\"Helper routine for 'find_bad_zeros'.\n",
    "    \n",
    "    This operates upon a single dataframe produced by 'groupby'. We expect an \n",
    "    additional column 'meter_id' which is a duplicate of 'meter' because groupby \n",
    "    eliminates the original one.\"\"\"\n",
    "    meter = Xy_subset.meter_id.iloc[0]\n",
    "    is_zero = Xy_subset.meter_reading == 0\n",
    "    if meter == 0:\n",
    "        # Electrical meters should never be zero. Keep all zero-readings in this table so that\n",
    "        # they will all be dropped in the train set.\n",
    "        return is_zero\n",
    "\n",
    "    transitions = (is_zero != is_zero.shift(1))\n",
    "    all_sequence_ids = transitions.cumsum()\n",
    "    ids = all_sequence_ids[is_zero].rename(\"ids\")\n",
    "    if meter in [2, 3]:\n",
    "        # It's normal for steam and hotwater to be turned off during the summer\n",
    "        keep = set(ids[(Xy_subset.timestamp < summer_start) |\n",
    "                       (Xy_subset.timestamp > summer_end)].unique())\n",
    "        is_bad = ids.isin(keep) & (ids.map(ids.value_counts()) >= min_interval)\n",
    "    elif meter == 1:\n",
    "        time_ids = ids.to_frame().join(Xy_subset.timestamp).set_index(\"timestamp\").ids\n",
    "        is_bad = ids.map(ids.value_counts()) >= min_interval\n",
    "\n",
    "        # Cold water may be turned off during the winter\n",
    "        jan_id = time_ids.get(0, False)\n",
    "        dec_id = time_ids.get(8283, False)\n",
    "        if (jan_id and dec_id and jan_id == time_ids.get(500, False) and\n",
    "                dec_id == time_ids.get(8783, False)):\n",
    "            is_bad = is_bad & (~(ids.isin(set([jan_id, dec_id]))))\n",
    "    else:\n",
    "        raise Exception(f\"Unexpected meter type: {meter}\")\n",
    "\n",
    "    result = is_zero.copy()\n",
    "    result.update(is_bad)\n",
    "    return result\n",
    "\n",
    "def find_bad_zeros(X, y):\n",
    "    \"\"\"Returns an Index object containing only the rows which should be deleted.\"\"\"\n",
    "    Xy = X.assign(meter_reading=y, meter_id=X.meter)\n",
    "    is_bad_zero = Xy.groupby([\"building_id\", \"meter\"]).apply(make_is_bad_zero)\n",
    "    return is_bad_zero[is_bad_zero].index.droplevel([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_bad_sitezero` identifies the \"known-bad\" electrical readings from the first 141 days of the data for site 0 (i.e. UCF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_sitezero(X):\n",
    "    \"\"\"Returns indices of bad rows from the early days of Site 0 (UCF).\"\"\"\n",
    "    return X[(X.timestamp < 3378) & (X.site_id == 0) & (X.meter == 0)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_bad_building1099` identifies the most absurdly high readings from building 1099. These are orders of magnitude higher than all data, and have been emperically seen in LB probes to be harmful outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_building1099(X, y):\n",
    "    \"\"\"Returns indices of bad rows (with absurdly high readings) from building 1099.\"\"\"\n",
    "    return X[(X.building_id == 1099) & (X.meter == 2) & (y > 3e4)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `find_bad_rows` combines all of the above together to allow you to do a one-line cleanup of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_rows(X, y):\n",
    "    return find_bad_zeros(X, y).union(find_bad_sitezero(X)).union(find_bad_building1099(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework\n",
    "The following code is taken from my previous kernel: [Strategy evaluation: What helps and by how much?](https://www.kaggle.com/purist1024/strategy-evaluation-what-helps-and-by-how-much). It is described in more detail there and so, in order to get to the point, we incorporate it here without the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "pd.set_option(\"max_columns\", 500)\n",
    "\n",
    "def input_file(file):\n",
    "    path = f\"../input/ashrae-energy-prediction/{file}\"\n",
    "    if not os.path.exists(path): return path + \".gz\"\n",
    "    return path\n",
    "\n",
    "def compress_dataframe(df):\n",
    "    result = df.copy()\n",
    "    for col in result.columns:\n",
    "        col_data = result[col]\n",
    "        dn = col_data.dtype.name\n",
    "        if dn == \"object\":\n",
    "            result[col] = pd.to_numeric(col_data.astype(\"category\").cat.codes, downcast=\"integer\")\n",
    "        elif dn == \"bool\":\n",
    "            result[col] = col_data.astype(\"int8\")\n",
    "        elif dn.startswith(\"int\") or (col_data.round() == col_data).all():\n",
    "            result[col] = pd.to_numeric(col_data, downcast=\"integer\")\n",
    "        else:\n",
    "            result[col] = pd.to_numeric(col_data, downcast='float')\n",
    "    return result\n",
    "\n",
    "def read_train():\n",
    "    df = pd.read_csv(input_file(\"train.csv\"), parse_dates=[\"timestamp\"])\n",
    "    df.timestamp = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
    "    return compress_dataframe(df)\n",
    "\n",
    "def read_building_metadata():\n",
    "    return compress_dataframe(pd.read_csv(\n",
    "        input_file(\"building_metadata.csv\")).fillna(-1)).set_index(\"building_id\")\n",
    "\n",
    "site_GMT_offsets = [-5, 0, -7, -5, -8, 0, -5, -5, -5, -6, -7, -5, 0, -6, -5, -5]\n",
    "\n",
    "def read_weather_train(fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    df = pd.read_csv(input_file(\"weather_train.csv\"), parse_dates=[\"timestamp\"])\n",
    "    df.timestamp = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
    "    if fix_timestamps:\n",
    "        GMT_offset_map = {site: offset for site, offset in enumerate(site_GMT_offsets)}\n",
    "        df.timestamp = df.timestamp + df.site_id.map(GMT_offset_map)\n",
    "    if interpolate_na:\n",
    "        site_dfs = []\n",
    "        for site_id in df.site_id.unique():\n",
    "            # Make sure that we include all possible hours so that we can interpolate evenly\n",
    "            site_df = df[df.site_id == site_id].set_index(\"timestamp\").reindex(range(8784))\n",
    "            site_df.site_id = site_id\n",
    "            for col in [c for c in site_df.columns if c != \"site_id\"]:\n",
    "                if add_na_indicators: site_df[f\"had_{col}\"] = ~site_df[col].isna()\n",
    "                site_df[col] = site_df[col].interpolate(limit_direction='both', method='linear')\n",
    "                # Some sites are completely missing some columns, so use this fallback\n",
    "                site_df[col] = site_df[col].fillna(df[col].median())\n",
    "            site_dfs.append(site_df)\n",
    "        df = pd.concat(site_dfs).reset_index()  # make timestamp back into a regular column\n",
    "    elif add_na_indicators:\n",
    "        for col in df.columns:\n",
    "            if df[col].isna().any(): df[f\"had_{col}\"] = ~df[col].isna()\n",
    "    return compress_dataframe(df).set_index([\"site_id\", \"timestamp\"])\n",
    "\n",
    "def combined_train_data(fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    Xy = compress_dataframe(read_train().join(read_building_metadata(), on=\"building_id\").join(\n",
    "        read_weather_train(fix_timestamps, interpolate_na, add_na_indicators),\n",
    "        on=[\"site_id\", \"timestamp\"]).fillna(-1))\n",
    "    return Xy.drop(columns=[\"meter_reading\"]), Xy.meter_reading\n",
    "\n",
    "def _add_time_features(X):\n",
    "    return X.assign(tm_day_of_week=((X.timestamp // 24) % 7), tm_hour_of_day=(X.timestamp % 24))\n",
    "\n",
    "class CatSplitRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model, col):\n",
    "        self.model = model\n",
    "        self.col = col\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.fitted = {}\n",
    "        importances = []\n",
    "        for val in X[self.col].unique():\n",
    "            X1 = X[X[self.col] == val].drop(columns=[self.col])\n",
    "            self.fitted[val] = clone(self.model).fit(X1, y.reindex_like(X1))\n",
    "            importances.append(self.fitted[val].feature_importances_)\n",
    "            del X1\n",
    "        fi = np.average(importances, axis=0)\n",
    "        col_index = list(X.columns).index(self.col)\n",
    "        self.feature_importances_ = [*fi[:col_index], 0, *fi[col_index:]]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        result = np.zeros(len(X))\n",
    "        for val in X[self.col].unique():\n",
    "            ix = np.nonzero((X[self.col] == val).to_numpy())\n",
    "            predictions = self.fitted[val].predict(X.iloc[ix].drop(columns=[self.col]))\n",
    "            result[ix] = predictions\n",
    "        return result\n",
    "\n",
    "categorical_columns = [\n",
    "    \"building_id\", \"meter\", \"site_id\", \"primary_use\", \"had_air_temperature\", \"had_cloud_coverage\",\n",
    "    \"had_dew_temperature\", \"had_precip_depth_1_hr\", \"had_sea_level_pressure\", \"had_wind_direction\",\n",
    "    \"had_wind_speed\", \"tm_day_of_week\", \"tm_hour_of_day\"\n",
    "]\n",
    "\n",
    "class LGBMWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, categorical_feature=None, **params):\n",
    "        self.model = LGBMRegressor(**params)\n",
    "        self.categorical_feature = categorical_feature\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        with warnings.catch_warnings():\n",
    "            cats = None if self.categorical_feature is None else list(\n",
    "                X.columns.intersection(self.categorical_feature))\n",
    "            warnings.filterwarnings(\"ignore\",\n",
    "                                    \"categorical_feature in Dataset is overridden\".lower())\n",
    "            self.model.fit(X, y, **({} if cats is None else {\"categorical_feature\": cats}))\n",
    "            self.feature_importances_ = self.model.feature_importances_\n",
    "            return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {**self.model.get_params(deep), \"categorical_feature\": self.categorical_feature}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        ctf = params.pop(\"categorical_feature\", None)\n",
    "        if ctf is not None: self.categorical_feature = ctf\n",
    "        self.model.set_params(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are simple variants of the ones above, but deal with loading in the test set. They could easily be refactored to share code with those functions, but we keep them separate for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test():\n",
    "    df = pd.read_csv(input_file(\"test.csv\"), parse_dates=[\"timestamp\"])\n",
    "    df.timestamp = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
    "    return compress_dataframe(df).set_index(\"row_id\")\n",
    "\n",
    "def read_weather_test(fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    df = pd.read_csv(input_file(\"weather_test.csv\"), parse_dates=[\"timestamp\"])\n",
    "    df.timestamp = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
    "    if fix_timestamps:\n",
    "        GMT_offset_map = {site: offset for site, offset in enumerate(site_GMT_offsets)}\n",
    "        df.timestamp = df.timestamp + df.site_id.map(GMT_offset_map)\n",
    "    if interpolate_na:\n",
    "        site_dfs = []\n",
    "        for site_id in df.site_id.unique():\n",
    "            # Make sure that we include all possible hours so that we can interpolate evenly\n",
    "            site_df = df[df.site_id == site_id].set_index(\"timestamp\").reindex(range(8784, 26304))\n",
    "            site_df.site_id = site_id\n",
    "            for col in [c for c in site_df.columns if c != \"site_id\"]:\n",
    "                if add_na_indicators: site_df[f\"had_{col}\"] = ~site_df[col].isna()\n",
    "                site_df[col] = site_df[col].interpolate(limit_direction='both', method='linear')\n",
    "                # Some sites are completely missing some columns, so use this fallback\n",
    "                site_df[col] = site_df[col].fillna(df[col].median())\n",
    "            site_dfs.append(site_df)\n",
    "        df = pd.concat(site_dfs).reset_index()  # make timestamp back into a regular column\n",
    "    elif add_na_indicators:\n",
    "        for col in df.columns:\n",
    "            if df[col].isna().any(): df[f\"had_{col}\"] = ~df[col].isna()\n",
    "    return compress_dataframe(df).set_index([\"site_id\", \"timestamp\"])\n",
    "\n",
    "def combined_test_data(fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    X = compress_dataframe(read_test().join(read_building_metadata(), on=\"building_id\").join(\n",
    "        read_weather_test(fix_timestamps, interpolate_na, add_na_indicators),\n",
    "        on=[\"site_id\", \"timestamp\"]).fillna(-1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit, predict, and submit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, read in the data, and identify the bad rows using the provided functions. Write out the bad rows in an index-per-line format for fast and easy re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = combined_train_data()\n",
    "\n",
    "bad_rows = find_bad_rows(X, y)\n",
    "pd.Series(bad_rows.sort_values()).to_csv(\"rows_to_drop.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the bad rows that we identified above, and then train the model using our favorite features and regressors. See [Strategy evaluation: What helps and by how much?](https://www.kaggle.com/purist1024/strategy-evaluation-what-helps-and-by-how-much) for information on the specific strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(index=bad_rows)\n",
    "y = y.reindex_like(X)\n",
    "\n",
    "# Additional preprocessing\n",
    "X = compress_dataframe(_add_time_features(X))\n",
    "X = X.drop(columns=\"timestamp\")  # Raw timestamp doesn't help when prediction\n",
    "y = np.log1p(y)\n",
    "\n",
    "model = CatSplitRegressor(\n",
    "    LGBMWrapper(random_state=0, n_jobs=-1, categorical_feature=categorical_columns), \"meter\")\n",
    "\n",
    "model.fit(X, y)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test set and predict meter readings. We must, of course, use exponentiation to convert our predictions back from log-scale to the desired kWh values. We also clip to a minimum of zero, since we know that there will be no negative readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_test_data()\n",
    "X = compress_dataframe(_add_time_features(X))\n",
    "X = X.drop(columns=\"timestamp\")  # Raw timestamp doesn't help when prediction\n",
    "\n",
    "predictions = pd.DataFrame({\n",
    "    \"row_id\": X.index,\n",
    "    \"meter_reading\": np.clip(np.expm1(model.predict(X)), 0, None)\n",
    "})\n",
    "\n",
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write the predictions out for submission. After that, it's Miller Time (tm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(\"submission.csv\", index=False, float_format=\"%.4f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
